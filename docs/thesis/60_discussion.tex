\section{Discussion and Conclusions }

% ------------------------------ 6.1 Summary of Findings------------------------------ %

\subsection{Summary of Findings}


\begin{textblock}
The main goal of the project is to build a boat simulator capable of providing realistic conditions for successfully training DRL agents, starting from the task of autonomous GPS waypoint navigation. 

The second goal of the project is to evaluate the level of reproducibility of the results, given the noisy learning environment produced by the fluid simulation.

The third, side goal, of the project is to provide an initial assessment of suitable algorithms and other hyperparameters that can serve as a base for future developments.
\end{textblock}



\begin{textblock}
{\bf Training }

The overall results confirm that the simulator's suitable environment   contributes to the expansion of Deep Reinforcement Learning (DRL) algorithms in the domain of autonomous boat navigation. By designing and implementing DRL agents to autonomously navigate and learn in the simulated environment, the project demonstrates both the capabilities of the simulator and the feasibility and partially assesses the effectiveness of using DRL techniques for autonomous waypoint navigation tasks.
\end{textblock}




\begin{textblock}
{\bf Stability }

The simulator demonstrates it is capable of a good level of reproducibility, despite the very noisy environment generated by the physics of the water. While my testing was of an empirical nature, and cannot really assess the level of reproducibility of its precision, the environment seems to yield similar overall results across multiple runs or experiments. 

More in general, despite the complex and challenging nature of the environment, the simulator proves to be suitable for training DRL agents, enabling them to learn and adapt effectively. These positive experimental results highlight the reliability and effectiveness of the simulator, emphasizing its potential for facilitating the development of robust and capable DRL agents.

Further consideration can be made by looking at Test 2 and Test 3 is possible to notice how in Test 3 there appears much less variance between episodes. During the development of the simulator, I noticed that the behaviour of the water dramatically changed by altering a seemingly unconnected parameter, the current Frame-Per-Second (FPS) of the scene, which was later confirmed by the ZibaAI team. 

Despite setting it to a fixed value inside the simulator, I’m induced to believe that a temporary decrease in performance during the simulation may induce some variances in the dynamics of the liquid, which then introduces variance in the scene. The reason is that the results of Test 3 were entirely produced by night, where most of the experiments from Test 2 were conducted during the day, while I was using the computer.

While intuition has to be verified with the ZibraAI team, in the meantime it is recommended to keep the load stable on the computer running the simulator. 

Talking about stability, it is worth mentioning that the simulator itself has never crashed or frozen throughout the experiments.
\end{textblock}




\begin{textblock}
{\bf Sensors }

The current setup shows that autonomous waypoint navigation can be achieved solely by the use of GPS and Compass sensors. The GPS sensor provides precise location information. The Compass sensor determines the orientation and heading of the vehicle, which otherwise should be extrapolated by the agent by using GPS coordinates between subsequent steps.
\end{textblock}




\begin{textblock}
{\bf Algorithms }

The data also confirms that my initial assumptions were accurate: PPO is more challenging to train than REINFORCE and exhibits lower performance at the beginning of the training. The results show that PPO requires more training iterations to achieve comparable performance, indicating a slower convergence. Additionally, PPO demonstrates higher variance during the initial phase of the training, making the learning process less predictable and more unstable. The findings also highlight the sensitivity of PPO to hyperparameter tuning, further emphasizing its difficulty. In summary, the experimental results validate my suppositions, emphasizing the need for careful design and optimization when using PPO in reinforcement learning training.
\end{textblock}



% ------------------------------ 6.2 Limitations and Future Research Directions
 ------------------------------ %

\subsection{Limitations and Future Research Directions}


\begin{textblock}
{\bf Tasks }

The next step in the development of the simulator is to increase the number of available tasks and testing scenarios
\end{textblock}


\begin{itemize}
    \item {\bf GPS Waypoint navigation Randomized } \\
    By randomizing the starting position for the target and the position and orientation of the boat. The agent should be able to generalize the task.

    \item {\bf GPS Waypoint navigation with fixed submerged obstacles } \\
    This task requires adding the Sonar to the Action-state. By using the sonar the boat can learn to avoid submerged or partially submerged obstacles.
    
    
    \item {\bf GPS Waypoint navigation with the coastline as obstacle } \\
    This task requires adding the Sonar to the Action-state. By using the sonar the boat can learn to sense shores and docks.
    In combination with the reading coming from the visual bathymetry inside the simulator, the sonar will be also able to realize it is approaching the shore.
    
    
    \item {\bf GPS Waypoint navigation with water-level obstacles } \\
    This task requires adding a lidar or a camera ( or both ) to the Action-State. An ideal task is to assess how noisy the lidar or the camera is, on a constantly tilting object. The task can be randomized to increase generality.
    
    
    \item {\bf GPS Waypoint navigation Randomized with multiple boats } \\
    This task requires adding a lidar or a camera ( or both ) to the Action-State. By spawning multiple boats, in the same wide training area, each having a randomized starting point and target, it would be possible for the agents to learn about collision avoidance with other floating, moving obstacles. This setup will also allow us to leverage parallelism.
    
    \item {\bf Autonomous Search Tasks: Image } \\
    This task requires adding a camera ( or both ) to the Action-State.
    By extending the simulator it should be possible for an advanced agent to autonomously explore the environment searching for items, for example, a type of fish. The camera can be placed both under or above water. 
    
    \item {\bf Autonomous Search Tasks: Molecules } \\
    This task requires adding a molecular/particle sensor to the Action-State.
    By extending the simulator it should be possible for an advanced agent to autonomously explore the environment tracing for a substance, simulated using a sparse particle system. The sensor can be placed both under or above water.


\end{itemize}


\begin{textblock}
Implementing these new tasks and training effectively on them should allow for the agent to be tested in real life. The use of Safe DRL is particularly interesting for the task of collision avoidance.
Adverse weather conditions, like water currents and waves, can be added to all previous tasks.
\end{textblock}



\begin{textblock}
{\bf Safe DRL }

Another interesting evolution of this work is the use of Safe Deep Reinforcement Learning (Safe DRL), specifically Constrained Policy Optimization (CPO). CPO is a general-purpose policy search algorithm designed for constrained reinforcement learning tasks, where both a reward function and constraints are specified for the agent's behaviour \cite{cpo2017}.

CPO enables the training of neural network policies for high-dimensional control tasks while ensuring that the policy behaviour remains within the specified constraints throughout the training process. It introduces a novel theoretical result that establishes a bound between the expected returns of two policies and the average divergence between them. This bound forms the basis for the guarantees provided by CPO, ensuring that the policy remains close to the desired constraints. Thanks to this approach the reward shaping connected to critical actions, could be instead expressed with hard constraints.
\end{textblock}



\begin{textblock}
{\bf Future Projects }

The development of this multipurpose boat navigation simulator has the potential to significantly speed up the development of future projects based on autonomous boat navigation. Like the one recently terminated which would focus on water quality monitoring which was funded by the European Union to the University of Verona \cite{intcatch}\cite{intcatcheuro}. 

By creating a virtual environment for testing and experimentation, it becomes easy to rapidly iterate on algorithms and hyperparameter settings, without the need for physical boats and real-world resources. This allows for quicker development cycles, reduces costs, and accelerates the learning process. Additionally, the simulator's versatility makes it easy to adapt, or eventually extended for new applications.
\end{textblock}


\begin{textblock}
{\bf Parallelism }

Implementing parallelism by utilizing a multiagent environment seems to be the most promising next step. Parallelism enables multiple agents to interact and learn simultaneously, leading to faster and more efficient training. By running multiple agents in parallel, the training process can be accelerated, and the agents can learn from each other's experiences.

By combining the parallelism of the multiagent environment and utilizing PPO, the agent's training can be further optimized and serve as an ideal foundation for tackling more complex tasks\cite{kanervisto2020}.

The main limiting factor is the lack of support for Linux. While the team is confident to release it in the future, at the time of writing it is not yet available. This is generally not a favourable factor as traditionally all the software stack for RL runs on Linux. Practically limiting access to computational power.

In order to address the issue some preliminary tests have been done. I managed to use the Wine compatibility layer and actually run the simulator and it seems stable, however, the FPS are not nearly enough to be usable. 
From the initial finding, looking into DXVK support in the Wine environment is likely to yield good results. By using DXVK it would be possible to run it on Linux seamlessly, simplifying the whole process and allowing the use of dedicated servers. \cite{dxvksteamcommunity}\cite{dxvklc2020}.

Another possible solution that remains interesting for future iterations of this project is to publish the game on Valves’s Steam, an online gaming platform, that offers a Protons compatibility layer out of the box. 
\end{textblock}



\begin{textblock}
{\bf Reusability }

By training agents to learn navigation strategies through interactions with the environment, the project demonstrates the ability of DRL algorithms to adapt and learn in complex field scenarios.

Moreover, several components have been developed and manually tested  while developing the simulator and they have not been used or accessed during the training. Those sensors can enable new capabilities for the boat or they can be directly reused into the Unity3D project, contributing to the Unity ecosystem.

\end{textblock}




\begin{textblock}
{\bf Environment Challenges }

Several environmental elements have been disabled, like the collidable docks, and the Wave Generator. For example, in the case of the Wave Generator, the system has been tested and is ready to be used, however has proven to be too challenging for the agents trained during this work. The complexity introduced by wave generators and water currents often overwhelms the agent's learning capabilities and hinders its performance.
During the experiments, the wave generators, at times, ended up sinking the boat. While this correctly simulates real-life scenarios. From my observations it appears to be detrimental to the learning process of the agent, at this stage of advancement, therefore has not been included in this experimental setup.

\end{textblock}